---
title: "Algebra and Statistical Methods for Big Data with Bioconductor"
author: 
- name: Dolors Pelegrí
  affiliation: 
  - &uab Autonomous University of Barcelona (UAB)
  - &isglobal ISGlobal, Centre for Research in Environmental Epidemiology (CREAL)
  - &brge Bioinformatics Research Group in Epidemiolgy (BRGE)
- name: Juan R. Gonzalez
  affiliation: 
  - *isglobal
  - *brge
  - *uab
  email: juanr.gonzalez@isglobal.org
date: "`r Sys.Date()`"
package: "`r pkg_ver('BigDataStatMeth')`"
output: 
  BiocStyle::html_document:
    number_sections: true
    toc: yes
    fig_caption: yes
    toc_float: true
vignette: >
  %\VignetteIndexEntry{Algebra and Statistical Methods for Big Data with Bioconductor}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "", 
                      cache=TRUE, message = FALSE, 
                      out.width = 100
                      )
```


# Prerequisites


# Overview

<!-- AQUÍ FER SECCIONS PRINCIPALS AMB : 
  - Operacions bàsiques amb matrius:
      . Producte de matrius
      . Matriu x vector
      . Crossprod i tCrossprod
      . ...
  - Algebra lineal:
      . svd
      . inversa matrius
      . ...
  - Aplicació algebra lineal:
      . PCA
      . OLS-GRID
      . ...
-->

# Getting started

First, let us start by loading the required packages to describe the main capabilities of the package

```{r load}
library(BigDataStatMeth)
library(DelayedArray)
```

These other packages are required to reproduce this vignette

```{r load2}
library(microbenchmark)
```

# Matrix Multiplication

## Simple matrix multiplication

The function `blockmult()` performs a simple sequential matrix multiplication [**explica lo que es sequential matrix multiplication**]. The function allows the use of `DelayedArray` objects as well as any other R data type. 

Let us simulate to set of matrices to illustrate the use of the function accross the entire documment. First, we simulate a simple case with to matrices A and B with dimensions 500x500 and 500x750, respectively. Second, another example with dimensions 1000x10000 are use to evaluate the performance in large matrices. The examples with big datasets will be illustrated using real data belonging to different omic settings. We can simulate to matrices with the desired dimensions by

```{r mat_sim}
set.seed(123456)
n <- 500
p <- 750
A <- matrix(rnorm(n*n), nrow=n, ncol=n)
B <- matrix(rnorm(n*p), nrow=n, ncol=p)

n <- 1000
p <- 10000
Abig <- matrix(rnorm(n*n), nrow=n, ncol=n)
Bbig <- matrix(rnorm(n*p), nrow=n, ncol=p)
```

They can be converted into `DelayedArray` objects by simply

```{r getDelayed}
DA <- DelayedArray(A)
DB <- DelayedArray(B)
```

Matrix multiplication is then done by

```{r mat_mult}
AxB <- blockmult(A, B)
AxBDelay <- blockmult(DA, DB)
AxBDelay [1:5,1:5]
```

This verifies that computations are fine, even when comparing with matrix multiplication with R (i.e. using `%*%)

```{r check}
all.equal(AxB, AxBDelay)
all.equal(A%*%B, AxBDelay)
```

The process can be speed up by making computations in parallel using `paral=TRUE`. The algorithm is implemented ....  [**tienes que decir qué usa  .. si lo hace a bajo o alto nivel**]. [**¿usa todos los cores? - debería tener un parámetro**]  

```{r noblockmultparal}
AxB <- blockmult(A, B, paral = TRUE)
AxBDelay <- blockmult(DA, DB, paral = TRUE) 

all.equal(AxB, AxBDelay)
```

We can show that the parallel version really improves the computational speed and also how the blockmult function improves the R implementation.

```{r benchmark1}
microbenchmark(
  R = A%*%B,
  noparallel = blockmult(A, B),
  parallel = blockmult(A, B, paral=TRUE),
  times = 10)
```

## Block matrix multiplication

A block matrix or a partitioned matrix is a matrix that is interpreted as having been broken into sections called blocks or submatrices. Intuitively, a block matrix can be visualized as the original matrix with a collection of horizontal and vertical lines, which break it up, or partition it, into a collection of smaller matrices. More information can be found [here](https://en.wikipedia.org/wiki/Block_matrix). [**hay alguna ventaja de tener esto?** supongo que si, entonces hay que explicarlo]

Matrix multiplication using block matrices is implemented in the `blockmult()` function. It only requires to setting the argument `block_size` different from 0 [**¿se puede optimizar en función de los cores?**]

```{r blockmult }
AxB <- blockmult(A, B, block_size = 10)
AxBDelay <- blockmult(DA, DB, block_size = 10 )
```

As expected the results obtained using this procedure are the correct ones

```{r}
all.equal(AxBDelay,A%*%B)
all.equal(AxB, AxBDelay)
```

Note that when the argument `block_size` is larger than any of the dimensions of matrix A or B the `blocks_size` is set to `min(cols(A), rows(A), cols(B), rows(B))`. 

As in the case of using a simple matrix multiplication, one can make the  operations in parallel with `paral = TRUE` .

```{r blockmultparal}
AxB <- blockmult(A, B, block_size = 10, paral = TRUE)
AxBDelay <- blockmult(DA, DB, block_size = 10, paral = TRUE )

all.equal(AxBDelay,A%*%B)
all.equal(AxB, AxBDelay)
```

Here, we can compare the performace of this method with the one implemented in the simple case using big matrices. In a large benchmark we performed [**see ...**], the conclusion is that  can observe that ,,, [**lo pondremos ... si mejora con matrices grandes o qué**]

```{r benchmark2}
microbenchmark(
  noblockParal = blockmult(Abig, Bbig, paral = TRUE),
  blockParal = blockmult(Abig, Bbig, block_size = 100, paral=TRUE),
  times = 10)
```


# Cross-product and Transposed Cross-product

To perform a cross-product $C = A^t A$ you can call `bdcrossprod()`.

```{r crossprod }
n <- 500
m <- 250
A <- matrix(rnorm(n*m), nrow=n, ncol=m)
DA <- DelayedArray(A)

# Cross Product
cpA <- bdcrossprod(A, transposed = FALSE)  
cpDA <- bdcrossprod(DA)  # With DelayedArray data type and no transposed parameter

all.equal(cpDA, crossprod(A))
```

you may also set `transposed=TRUE` (default value transposed=false) to perform a Transposed Cross-product $C = A A^t$

```{r nocrossprod }
# Transposed Cross Product
tcpA <- bdcrossprod(A, transposed = TRUE)
tcpDA <- bdcrossprod(DA, transposed = TRUE) # With DelayeArray data type

all.equal(tcpDA, tcrossprod(A))

```

# Matrix Vector Multiplication 

You can perform a weighted cross-product $C = X^ t w X$ with `bdwcrossprod()` given a matrix X as argument and a vector or matrix of weights, w. 

## Weighted Cross-product and Weighted Transposed Cross-product

```{r wcrossprod }
n <- 250
X <- matrix(rnorm(n*n), nrow=n, ncol=n)
u <- runif(n)
w <- u * (1 - u)
DX <- DelayedArray(X)
Dw <- DelayedArray(as.matrix(w))
  
wcpX <- bdwproduct(X, w,"xwxt")
wcpDX <- bdwproduct(DX, Dw,"xwxt") # with DelayedArray

wcpDX[1:5,1:5]

all.equal( wcpDX, X%*%diag(w)%*%t(X) )

```

## Weighted Transposed Cross Product 

With argument `transposed=TRUE`, we can perform a transposed weighted cross-product $C = A w A^t$  

```{r wtcrossprod }

wtcpX <- bdwproduct(X, w,"xtwx")
wtcpDX <- bdwproduct(DX, Dw,"xtwx") # with DelayedArray

wtcpDX[1:5,1:5]

all.equal(wtcpDX, t(X)%*%diag(w)%*%X)

```

# Inverse Cholesky
<p> The Cholesky factorization is widely used for solving a system of linear equations whose coefficient matrix is symmetric and positive definite.</p>
<p style="text-align: center;">$A = LL^t = U^tU$ </p>
where $L$ is a lower triangular matrix and U is an upper triangular matrix.
To get the Inverse Cholesky we can use the function `bdInvCholesky()`


```{r invChols }

# Generate a positive definite matrix
Posdef <- function (n, ev = runif(n, 0, 10)) 
{
  Z <- matrix(ncol=n, rnorm(n^2))
  decomp <- qr(Z)
  Q <- qr.Q(decomp) 
  R <- qr.R(decomp)
  d <- diag(R)
  ph <- d / abs(d)
  O <- Q %*% diag(ph)
  Z <- t(O) %*% diag(ev) %*% O
  return(Z)
}

A <- Posdef(n = 500, ev = 1:500)
DA <- DelayedArray(A)

invchol <- bdInvCholesky(A)
Dinvchol <- bdInvCholesky(DA)

round(invchol[1:5,1:5],8)

all.equal(Dinvchol, solve(A))

```

# SVD Decomposition

<p>The singular-value decomposition of an $mxn$ real or complex matrix $A$ is a factorization of the form :</p>
<p style="text-align: center;">$U\Sigma { V }^{ T }$</p>
where : 
$U$ is a $mxm$ real or complex unitary matrix
$\Sigma $ is a $mxn$ rectangular diagonal matrix with non-negative real numbers on the diagonal
$V$ is a $nxn$ real or complex unitary matrix.
The diagonal entries $\sigma_i$ of $\Sigma$ are known as the singular values of $A$
The columns of $U$ are called the left-singular vectors of $A$
The columns of $V$ are called the right-singular vectorsof $A$

## svd

The svd decomposition can be performed with the function `bdSVD()`, with bdSVD you can only perform a svd from real matrix $A$.  Given matrix A as argument 

```{r svd_default}
n <- 500
A <- matrix(rnorm(n*n), nrow=n, ncol=n)
DA <- DelayedArray(A)

bsvd <- bdSVD(A)
Dbsvd <- bdSVD(DA)

bsvd$d[1:10]
bsvd$u[1:5,1:5]
bsvd$v[1:5,1:5]

all.equal( sqrt( svd( tcrossprod( scale(A) ) )$d[1:10] ), bsvd$d[1:10] ) 
all.equal( sqrt( svd( tcrossprod( scale(A) ) )$d[1:10] ), Dbsvd$d[1:10] )

```

you get the $\sigma_i$, $U$ and $V$ of normalized matrix $A$, if you want to perform the SVD from not normalized matrix $A$ then you have to set the parameter `normalize = false`

```{r svd_nonorm}
bsvd <- bdSVD(A, normalize = FALSE)
Dbsvd <- bdSVD(DA, normalize = FALSE)

bsvd$d[1:10]
bsvd$u[1:5,1:5]
bsvd$v[1:5,1:5]

all.equal( sqrt(svd(tcrossprod(A))$d[1:10]), bsvd$d[1:10] ) 
all.equal( sqrt(svd(tcrossprod(A))$d[1:10]), Dbsvd$d[1:10] )
  
```



# LOOE - Leave One Out Error


# Ols-grid







# Session information

```{r sesinfo }
sessionInfo()
```

